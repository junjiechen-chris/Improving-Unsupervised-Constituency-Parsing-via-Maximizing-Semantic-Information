{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a place holder for my code release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from parsing_by_maxseminfo import parser\n",
    "import sys\n",
    "from parsing_by_maxseminfo import parser\n",
    "\n",
    "# necessary to use prepackaged data\n",
    "sys.modules['parser'] = parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/projects/tnpcfg-release/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(conf='/home/chris/projects/tnpcfg-release/config/pas-grammar/english-ew-reward-tbtok-idf/npcfg_nt60_t120_en.spacy-10k-merged-0pas-fast-6-3-rlstart0.yaml', use_tf32=False, rank=0, ngpu=1, max_pasdata_lendiff=-1, max_bandwidth=4, pas_subsample_count=-1, alignment_coefficient=-1, adversarial_coefficient=-1, batch_size=4, debug=False, flag_use_spacy_preprocessing=False, langstr='english', use_ppl_loss=False, span_repr_mode='bge-m3', remark='none', forbid_bn=False, forbid_merged_nll=False, use_normalized_term_mlp=False, use_onesided=False, dropout=-1, force_fp32=False, max_length=40, wandb_tags=None, val_check_interval=5000, ckpt_dir='./checkpoints/', flag_curriculum_learning=False, flag_use_separate_nll_path=False, unset_logppl=False, unset_nll_weighing=False, set_fast_model=False, set_mode_offending_spans=False, ckpt=None, unset_renormalizing_marginals=False, set_lr=-1, unset_bert_mode=False, set_pas_suppression=False, wandb_project='Spanoverlap-PCFG', set_hit_count_threshold=-1, preprocessing_pas_subsample_count=10000, set_add_sentence_level_span=False, set_spancomp_coefficient=-1, flag_compute_relative_frequency=False, set_training_mode='nll', set_min_span_reward=-4.0, distinct_prompt=False, vocab_max_size=10000, vocab_min_freq=5, mode_reward=None, set_reward_normalization=False, unset_preprocessing_spacy=False, unset_ptb_mode=True, set_include_unary=True, set_sample_epsilon=-1, continue_from=None, set_treecrf_samples=-1, set_mode_reward='log_tfidf', set_flag_use_pos_unks=False, run_val_only=False, log_tree_path=None, analysis_mode=False, corr_mode=False, output_detailed=False, set_supervised_mode=False, detailed_corr_mode=False, unset_baseline=False, use_pcfg_samples=False, set_normalize_pcfg_conditional_logprob=False)\n",
      "english\n",
      "loading from  /data/chris/projects/tnpcfg-release/data/english/ptb_en-full.gd_instruction.batch.gpt4omini-ew-exp-tbtok-idf\n",
      "Preparing datasets with 10000 PAS samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import parsing_by_maxseminfo.utils.prep\n",
    "import argparse\n",
    "import yaml \n",
    "import os\n",
    "from easydict import EasyDict as edict\n",
    "import torch\n",
    "from transformers import TrainingArguments \n",
    "import sys\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import lightning\n",
    "\n",
    "# lightning.seed_everything(0)\n",
    "\"\"\"\n",
    "Training mode can be selected from one of the below\n",
    "rl: SemInfo mean-baseline training with CRF as explained in the main text\n",
    "nll: LL training as explained in the main text\n",
    "a2c: Stepwise SemInfo training with CRF as explained in Appendix A1\n",
    "a2c_v0: Posterior V0 training with CRF\n",
    "ta2c_rules: Stepwise SemInfo training with PCFG\n",
    "ta2c: Posterior V0 training with PCFG\n",
    "tavg: Posterior mean-baseline training with PCFG\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "training_mode = \"a\"\n",
    "assert training_mode in [\"rl\", \"nll\", \"a2c\", \"ta2c\", \"ta2c_rules\", \"a2c_v0\", \"tavg\"]\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "f_config = \"/home/chris/projects/tnpcfg-release/config/pas-grammar/english-ew-reward-tbtok-idf/npcfg_nt60_t120_en.spacy-10k-merged-0pas-fast-6-3-rlstart0.yaml\"\n",
    "input_args = [\n",
    "    f\"-c={f_config}\",\n",
    "    \"--max_length=40\", \n",
    "    f\"--set_training_mode={training_mode}\",\n",
    "    \"--set_min_span_reward=-4\", \n",
    "    \"--batch_size=4\",\n",
    "    \"--unset_ptb_mode\",\n",
    "    \"--ckpt_dir=./checkpoints/\",\n",
    "    \"--set_mode_reward=log_tfidf\",\n",
    "    \"--set_include_unary\",\n",
    "    # \"--use_pcfg_samples\",\n",
    "    \"--langstr=english\",\n",
    "]\n",
    "\n",
    "\n",
    "# %%\n",
    "from parsing_by_maxseminfo.utils.myargparse import get_argsndevice\n",
    "\n",
    "# import train\n",
    "args, device = get_argsndevice(input_args)\n",
    "\n",
    "\n",
    "from parsing_by_maxseminfo.parser.helper.pas_grammar_data_helper import (\n",
    "    DataModuleForPASCtrlPCFGReward,\n",
    ")\n",
    "\n",
    "derivative = args.model.model_name.split(\"-\")[1]\n",
    "dst = DataModuleForPASCtrlPCFGReward(\n",
    "        hparams=args,\n",
    "        langstr=args.langstr,\n",
    "        use_cache=True,\n",
    "        max_size=10000,\n",
    "        merge_pas_data=False,\n",
    "        pas_subsample=args.preprocessing_pas_subsample_count,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./checkpoints/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.ckpt_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on vocab of size 10020\n",
      "launching ['NPCFGA2C', 'FixedCostReward']\n",
      "launching NPCFGA2C FixedCostReward\n",
      "Constructing LitNPCFGFixedCost with experimental config {'alignment_coefficient': 1.0, 'adversarial_coefficient': 0.0, 'pas_subsample_count': 0, 'renormalizing_marginals': False, 'weigh_nll_loss': True, 'suppress_pas_contrib': False, 'flag_curriculum_learning': False, 'mode': 'nll', 'hit_count_threshold': 2, 'activation_flood': 0.001, 'mode_offending_spans': True, 'spancomp_loss_weight': 4.0, 'rl_warmup_steps': 5000, 'rl_start_step': 0, 'rl_initial_coeff': 0.0, 'rl_target_coeff': 1.0, 'rl_len_norm': False, 'apply_mean_baseline': True, 'maxent_initial_coeff': -0.01, 'maxent_target_coeff': -0.01, 'mode_reward': 'log_tfidf', 'min_span_reward': -4.0, 'include_unary': True, 'supervised_mode': False, 'sample_mode': 'crf'}\n"
     ]
    }
   ],
   "source": [
    "word_vocab = dst.word_vocab\n",
    "print(\"working on vocab of size\", word_vocab.vocab_size)\n",
    "\n",
    "from parsing_by_maxseminfo.parser.model.C_PCFG import CompoundPCFGBaseline, CompoundPCFGPairwise\n",
    "from parsing_by_maxseminfo.parser.model.TN_PCFG import TNPCFGBaseline\n",
    "\n",
    "basemodel = args.model.model_name.split(\"-\")[0]\n",
    "from parsing_by_maxseminfo.parser.lightning_wrapper.LitNPCFG import (\n",
    "    LitXNPCFGFCReward,\n",
    ")\n",
    "\n",
    "print(f\"launching {args.model.model_name.split('-')}\")\n",
    "\n",
    "if basemodel in [\"SNPCFG\", \"TNPCFG\", \"NPCFG\", \"CPCFG\", \"SCPCFG\", \"SNPCFGA2C\", \"NPCFGA2C\", \"CPCFGA2C\"]:\n",
    "    # raise NotImplementedError(\"No plan for TNPCFG experiments so fat\")\n",
    "    derivative = args.model.model_name.split(\"-\")[1]\n",
    "    print(f\"launching {basemodel} {derivative}\")\n",
    "    model = LitXNPCFGFCReward(\n",
    "            basemodel,\n",
    "            args.model,\n",
    "            word_vocab.vocab_size,\n",
    "            args.experimental,\n",
    "            args.optimizer,\n",
    "            args.langstr,\n",
    "        )\n",
    "    \n",
    "    # model = TNPCFGFixedCost(args.model, word_vocab.vocab_size, span_repr_mode=\"em\", langstr = 'german').to(device)\n",
    "else:\n",
    "    raise NotImplementedError(f\"{args.model.model_name} is not allowed\")\n",
    "\n",
    "# # %%\n",
    "# args.test.batch_size = 4\n",
    "# args.train.batch_size =4\n",
    "\n",
    "\n",
    "# import Stemmer\n",
    "\n",
    "# print(args.langstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# stemmer = Stemmer.Stemmer(args.langstr)\n",
    "class IdentityStemmer:\n",
    "    def stemWords(self, words):\n",
    "        return words\n",
    "\n",
    "    def stemWord(self, word):\n",
    "        return word\n",
    "\n",
    "\n",
    "import lightning.pytorch as L\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.progress import TQDMProgressBar\n",
    "\n",
    "# Setup early stopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val/sentence_f1\",  # Metric to monitor\n",
    "    min_delta=0.002,  # Minimum change to qualify as an improvement\n",
    "    patience=args.train.patience,  # Number of epochs with no improvement after which training will be stopped\n",
    "    verbose=True,\n",
    "    mode=\"max\",  # Minimize the monitored metric (use 'max' for metrics like accuracy)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader: add_sentence_level_span: False\n",
      "train loader: reward mode: log_tfidf\n",
      "dev full loader: add_sentence_level_span: False\n",
      "train loader: reward mode: log_tfidf\n",
      "finished pruning dataset, current dataset length 1690\n",
      "sampling: current dataset size:  1690\n",
      "Train Iter: add_sentence_level_span False\n",
      "Train Iter: mode_offending_spans True\n",
      "sampling: current dataset size:  2412\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "train_dl, _ = dst.train_dataloader(\n",
    "        # \"english\",\n",
    "        # args.langstr,\n",
    "        \"null\",\n",
    "        max_len=40,\n",
    "        min_len=3,\n",
    "        device=device,\n",
    "        pas_subsample_count=args.experimental.pas_subsample_count,\n",
    "        flag_curriculum_learning=(\n",
    "            args.experimental.flag_curriculum_learning\n",
    "            if hasattr(args.experimental, \"flag_curriculum_learning\")\n",
    "            else False\n",
    "        ),\n",
    "        mode_offending_spans=(\n",
    "            args.experimental.mode_offending_spans\n",
    "            if hasattr(args.experimental, \"mode_offending_spans\")\n",
    "            else False\n",
    "        ),\n",
    "        bert_mode=(\n",
    "            args.model.bert_mode if hasattr(args.model, \"bert_mode\") else \"disabled\"\n",
    "        ),\n",
    "        add_sentence_level_span=(\n",
    "            args.experimental.add_sentence_level_span\n",
    "            if hasattr(args.experimental, \"add_sentence_level_span\")\n",
    "            else False\n",
    "        ),\n",
    "        min_span_reward=args.experimental.min_span_reward,  # min span reward must be specified\n",
    "        mode_reward=(\n",
    "            args.experimental.mode_reward\n",
    "            if hasattr(args.experimental, \"mode_reward\")\n",
    "            else \"none\"\n",
    "        ),\n",
    "        supervised_mode=(\n",
    "            args.experimental.supervised_mode\n",
    "            if hasattr(args.experimental, \"supervised_mode\")\n",
    "            else False\n",
    "        ),\n",
    "    )\n",
    "\n",
    "val_dl, _ = dst.dev_full_dataloader(\n",
    "    args.langstr,\n",
    "    max_len=100000,\n",
    "    min_len=2,\n",
    "    device=device,\n",
    "    flag_respect_ptb_boundary=True,\n",
    "    min_span_reward=args.experimental.min_span_reward,\n",
    "    mode_reward=(\n",
    "        args.experimental.mode_reward\n",
    "        if hasattr(args.experimental, \"mode_reward\")\n",
    "        else \"none\"\n",
    "    ),\n",
    "    # flag_respect_ptb_boundary=args.experimental.flag_respect_ptb_boundary if hasattr(args.experimental, 'flag_respect_ptb_boundary') else False,\n",
    ")\n",
    "\n",
    "test_dl, _ = dst.test_dataloader(\n",
    "    args.langstr,\n",
    "    max_len=1000000,\n",
    "    min_len=2,\n",
    "    device=device,\n",
    "    # flag_respect_ptb_boundary=args.experimental.flag_respect_ptb_boundary if hasattr(args.experimental, 'flag_respect_ptb_boundary') else False,\n",
    "    flag_respect_ptb_boundary=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'target_form_array': [['influential',\n",
       "    'members',\n",
       "    'of',\n",
       "    'the',\n",
       "    'house',\n",
       "    'ways',\n",
       "    'and',\n",
       "    'means',\n",
       "    'committee',\n",
       "    'introduced',\n",
       "    'legislation',\n",
       "    'that',\n",
       "    'would',\n",
       "    'restrict',\n",
       "    'how',\n",
       "    'the',\n",
       "    'new',\n",
       "    'savings-and-loan',\n",
       "    'bailout',\n",
       "    'agency',\n",
       "    'can',\n",
       "    'raise',\n",
       "    'capital',\n",
       "    'creating',\n",
       "    'another',\n",
       "    'potential',\n",
       "    'obstacle',\n",
       "    'to',\n",
       "    'the',\n",
       "    'government',\n",
       "    \"'s\",\n",
       "    'sale',\n",
       "    'of',\n",
       "    'sick',\n",
       "    'thrifts'],\n",
       "   ['we',\n",
       "    \"'re\",\n",
       "    'saying',\n",
       "    'the',\n",
       "    'worst',\n",
       "    'thing',\n",
       "    'that',\n",
       "    'anyone',\n",
       "    'can',\n",
       "    'do',\n",
       "    'is',\n",
       "    'to',\n",
       "    'see',\n",
       "    'the',\n",
       "    'market',\n",
       "    'go',\n",
       "    'down',\n",
       "    'and',\n",
       "    'dump',\n",
       "    'everything',\n",
       "    'which',\n",
       "    'just',\n",
       "    'drives',\n",
       "    'the',\n",
       "    'prices',\n",
       "    'down',\n",
       "    'further',\n",
       "    'says',\n",
       "    'john',\n",
       "    'lampe',\n",
       "    'painewebber',\n",
       "    \"'s\",\n",
       "    'director',\n",
       "    'of',\n",
       "    'advertising'],\n",
       "   ['at',\n",
       "    'jefferies',\n",
       "    \"'\",\n",
       "    'trading',\n",
       "    'room',\n",
       "    'on',\n",
       "    'finsbury',\n",
       "    'circus',\n",
       "    'a',\n",
       "    'stately',\n",
       "    'circle',\n",
       "    'at',\n",
       "    'the',\n",
       "    'edge',\n",
       "    'of',\n",
       "    'the',\n",
       "    'financial',\n",
       "    'district',\n",
       "    'desktop',\n",
       "    'computer',\n",
       "    'screens',\n",
       "    'displayed',\n",
       "    'the',\n",
       "    'london',\n",
       "    'market',\n",
       "    \"'s\",\n",
       "    'major',\n",
       "    'barometer',\n",
       "    'the',\n",
       "    'financial',\n",
       "    'times-stock',\n",
       "    'exchange',\n",
       "    'N',\n",
       "    'share',\n",
       "    'index'],\n",
       "   ['in',\n",
       "    'a',\n",
       "    'letter',\n",
       "    'to',\n",
       "    'subcommittee',\n",
       "    'chairman',\n",
       "    'james',\n",
       "    'oberstar',\n",
       "    'd.',\n",
       "    'minn.',\n",
       "    'mr.',\n",
       "    'trump',\n",
       "    'criticized',\n",
       "    'the',\n",
       "    'bill',\n",
       "    'as',\n",
       "    'an',\n",
       "    'explicit',\n",
       "    'effort',\n",
       "    'to',\n",
       "    'thwart',\n",
       "    'his',\n",
       "    'bid',\n",
       "    'for',\n",
       "    'amr',\n",
       "    'and',\n",
       "    'said',\n",
       "    'it',\n",
       "    'contributed',\n",
       "    'to',\n",
       "    'the',\n",
       "    'collapse',\n",
       "    'of',\n",
       "    'the',\n",
       "    'deal']],\n",
       "  'target_id_array': tensor([[5098,  488,   22,   20,  188, 1159,   26,  842,  405, 1238,  919,   29,\n",
       "             60, 4053,  255,   20,   53, 5171, 2313,  314,  108,  657,  200, 2496,\n",
       "            225,  640, 5728,   23,   20,  122,   27,  241,   22, 4493, 1880],\n",
       "          [  83,  289,  752,   20, 2314,  890,   29, 1440,  108,  106,   30,   23,\n",
       "            380,   20,   63,  389,  136,   26, 4702, 1459,   59,  161, 2023,   20,\n",
       "            134,  136,  403,   62,  425,    1, 1561,   27,  292,   22,  641],\n",
       "          [  35, 7498,  153,  101, 1365,   33,    1, 5367,   24,    1, 5251,   35,\n",
       "             20, 3137,   22,   20,  164,  871, 5579,  253, 5269, 7359,   20,  441,\n",
       "             63,   27,  155, 7458,   20,  164, 9203,  130,   21,   80,  260],\n",
       "          [  25,   24, 1104,   23, 2563,  158,  607,    1,  702, 7674,   41, 2486,\n",
       "           3078,   20,  290,   37,   49, 9770,  680,   23, 9538,   66,  254,   28,\n",
       "           2124,   26,   32,   31, 1255,   23,   20, 1535,   22,   20,  496]],\n",
       "         device='cuda:3'),\n",
       "  'target_len_array': tensor([35, 35, 35, 35]),\n",
       "  'reward': tensor([[[0.0000, 0.0000, 4.6931,  ..., 0.0000, 0.0000, 4.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "  \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 4.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "  \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 4.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "  \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 4.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           ...,\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "         device='cuda:3'),\n",
       "  'encoded_input': None,\n",
       "  'competing_span_mask': None,\n",
       "  'tree_compatible_masks': None,\n",
       "  'pas_indicator': tensor([0, 0, 0, 0], device='cuda:3'),\n",
       "  'gold_tree_mask': None},\n",
       " {'gold_tree': [[(0, 35, 'TOP'),\n",
       "    (0, 35, 'S'),\n",
       "    (0, 9, 'NP'),\n",
       "    (0, 2, 'NP'),\n",
       "    (2, 9, 'PP'),\n",
       "    (3, 9, 'NP'),\n",
       "    (9, 35, 'VP'),\n",
       "    (10, 35, 'NP'),\n",
       "    (10, 11, 'NP'),\n",
       "    (11, 35, 'SBAR'),\n",
       "    (11, 12, 'WHNP'),\n",
       "    (12, 35, 'S'),\n",
       "    (12, 35, 'VP'),\n",
       "    (13, 35, 'VP'),\n",
       "    (14, 23, 'SBAR'),\n",
       "    (14, 15, 'WHADVP'),\n",
       "    (15, 23, 'S'),\n",
       "    (15, 20, 'NP'),\n",
       "    (20, 23, 'VP'),\n",
       "    (21, 23, 'VP'),\n",
       "    (22, 23, 'NP'),\n",
       "    (23, 35, 'S'),\n",
       "    (23, 35, 'VP'),\n",
       "    (24, 35, 'NP'),\n",
       "    (24, 27, 'NP'),\n",
       "    (27, 35, 'PP'),\n",
       "    (28, 35, 'NP'),\n",
       "    (28, 32, 'NP'),\n",
       "    (28, 31, 'NP'),\n",
       "    (32, 35, 'PP'),\n",
       "    (33, 35, 'NP')],\n",
       "   [(0, 35, 'TOP'),\n",
       "    (0, 35, 'SINV'),\n",
       "    (0, 27, 'S'),\n",
       "    (0, 1, 'NP'),\n",
       "    (1, 27, 'VP'),\n",
       "    (2, 27, 'VP'),\n",
       "    (3, 27, 'SBAR'),\n",
       "    (3, 27, 'S'),\n",
       "    (3, 10, 'NP'),\n",
       "    (3, 6, 'NP'),\n",
       "    (6, 10, 'SBAR'),\n",
       "    (6, 7, 'WHNP'),\n",
       "    (7, 10, 'S'),\n",
       "    (7, 8, 'NP'),\n",
       "    (8, 10, 'VP'),\n",
       "    (9, 10, 'VP'),\n",
       "    (10, 27, 'VP'),\n",
       "    (11, 27, 'S'),\n",
       "    (11, 27, 'VP'),\n",
       "    (12, 27, 'VP'),\n",
       "    (13, 27, 'S'),\n",
       "    (13, 15, 'NP'),\n",
       "    (15, 27, 'VP'),\n",
       "    (15, 17, 'VP'),\n",
       "    (16, 17, 'ADVP'),\n",
       "    (18, 20, 'VP'),\n",
       "    (19, 20, 'NP'),\n",
       "    (20, 27, 'SBAR'),\n",
       "    (20, 21, 'WHNP'),\n",
       "    (21, 27, 'S'),\n",
       "    (21, 27, 'VP'),\n",
       "    (21, 22, 'ADVP'),\n",
       "    (23, 25, 'NP'),\n",
       "    (25, 26, 'ADVP'),\n",
       "    (26, 27, 'ADVP'),\n",
       "    (27, 28, 'VP'),\n",
       "    (28, 35, 'NP'),\n",
       "    (28, 30, 'NP'),\n",
       "    (30, 35, 'NP'),\n",
       "    (30, 33, 'NP'),\n",
       "    (30, 32, 'NP'),\n",
       "    (33, 35, 'PP'),\n",
       "    (34, 35, 'NP')],\n",
       "   [(0, 35, 'TOP'),\n",
       "    (0, 35, 'S'),\n",
       "    (0, 18, 'PP'),\n",
       "    (0, 18, 'NP'),\n",
       "    (0, 5, 'NP'),\n",
       "    (0, 3, 'NP'),\n",
       "    (5, 18, 'PP'),\n",
       "    (6, 18, 'NP'),\n",
       "    (6, 8, 'NP'),\n",
       "    (8, 18, 'NP'),\n",
       "    (8, 11, 'NP'),\n",
       "    (11, 18, 'PP'),\n",
       "    (12, 18, 'NP'),\n",
       "    (12, 14, 'NP'),\n",
       "    (14, 18, 'PP'),\n",
       "    (15, 18, 'NP'),\n",
       "    (18, 21, 'NP'),\n",
       "    (21, 35, 'VP'),\n",
       "    (22, 35, 'NP'),\n",
       "    (22, 28, 'NP'),\n",
       "    (22, 26, 'NP'),\n",
       "    (28, 35, 'NP')],\n",
       "   [(0, 35, 'TOP'),\n",
       "    (0, 35, 'S'),\n",
       "    (0, 10, 'PP'),\n",
       "    (0, 10, 'NP'),\n",
       "    (0, 3, 'NP'),\n",
       "    (3, 10, 'PP'),\n",
       "    (4, 10, 'NP'),\n",
       "    (4, 8, 'NP'),\n",
       "    (8, 10, 'PRN'),\n",
       "    (8, 9, 'NP'),\n",
       "    (9, 10, 'NP'),\n",
       "    (10, 12, 'NP'),\n",
       "    (12, 35, 'VP'),\n",
       "    (12, 25, 'VP'),\n",
       "    (13, 15, 'NP'),\n",
       "    (15, 25, 'PP'),\n",
       "    (16, 25, 'NP'),\n",
       "    (19, 25, 'S'),\n",
       "    (19, 25, 'VP'),\n",
       "    (20, 25, 'VP'),\n",
       "    (21, 25, 'NP'),\n",
       "    (21, 23, 'NP'),\n",
       "    (23, 25, 'PP'),\n",
       "    (24, 25, 'NP'),\n",
       "    (26, 35, 'VP'),\n",
       "    (27, 35, 'SBAR'),\n",
       "    (27, 35, 'S'),\n",
       "    (27, 28, 'NP'),\n",
       "    (28, 35, 'VP'),\n",
       "    (29, 35, 'PP'),\n",
       "    (30, 35, 'NP'),\n",
       "    (30, 32, 'NP'),\n",
       "    (32, 35, 'PP'),\n",
       "    (33, 35, 'NP')]]})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "best_sf1_checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=4,\n",
    "    monitor=\"val/sentence_f1\",\n",
    "    \n",
    "    mode=\"max\",\n",
    "    dirpath=args.ckpt_dir,\n",
    "    filename=\"ckpt-sf1_{val/sentence_f1:.2f}\",\n",
    ")\n",
    "saveall_checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=-1,\n",
    "    dirpath=args.ckpt_dir,\n",
    "    filename=\"ckpt-step_{step}\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsing_by_maxseminfo.parser.lightning_wrapper.scheduler import WarmupScheduler\n",
    "\n",
    "rl_coeff_scheduler = WarmupScheduler(\n",
    "    warmup_steps=(\n",
    "        args.experimental.rl_warmup_steps\n",
    "        if hasattr(args.experimental, \"rl_warmup_steps\")\n",
    "        else 10000\n",
    "    ),\n",
    "    coeff_name=\"rl_coeff\",\n",
    "    initial_coeff=(\n",
    "        args.experimental.rl_initial_coeff\n",
    "        if hasattr(args.experimental, \"rl_initial_coeff\")\n",
    "        else 0.0\n",
    "    ),\n",
    "    start_step=(\n",
    "        args.experimental.rl_start_step\n",
    "        if hasattr(args.experimental, \"rl_start_step\")\n",
    "        else 20000\n",
    "    ),\n",
    "    target_coeff=(\n",
    "        args.experimental.rl_target_coeff\n",
    "        if hasattr(args.experimental, \"rl_target_coeff\")\n",
    "        else 0.3\n",
    "    ),\n",
    ")\n",
    "\n",
    "maxent_scheduler = WarmupScheduler(\n",
    "    warmup_steps=(\n",
    "        args.experimental.maxent_warmup_steps\n",
    "        if hasattr(args.experimental, \"maxent_warmup_steps\")\n",
    "        else 1\n",
    "    ),\n",
    "    coeff_name=\"maxent_coeff\",\n",
    "    initial_coeff=(\n",
    "        args.experimental.maxent_initial_coeff\n",
    "        if hasattr(args.experimental, \"maxent_initial_coeff\")\n",
    "        else 0.5\n",
    "    ),\n",
    "    start_step=(\n",
    "        args.experimental.maxent_start_step\n",
    "        if hasattr(args.experimental, \"maxent_start_step\")\n",
    "        else 0.0\n",
    "    ),\n",
    "    target_coeff=(\n",
    "        args.experimental.maxent_target_coeff\n",
    "        if hasattr(args.experimental, \"maxent_target_coeff\")\n",
    "        else 0.5\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 100\n",
    "min_steps = 100\n",
    "val_check_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/chris/projects/tnpcfg-release/venv/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /data/chris/projects/tnpcfg-release/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type                         | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model | NeuralPCFGFixedCostRewardA2C | 24.5 M | train\n",
      "---------------------------------------------------------------\n",
      "24.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.5 M    Total params\n",
      "98.046    Total estimated model params size (MB)\n",
      "32        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Adam optimizer\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  5.61it/s]total samples 8.0\n",
      "dst_size 8\n",
      "finished pruning dataset                                                   \n",
      "Constructing shuffled batches with 50 epoches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Iter: add_sentence_level_span False\n",
      "Train Iter: mode_offending_spans True\n",
      "Epoch 0: |          | 0/? [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/chris/projects/tnpcfg-release/parsing_by_maxseminfo/parser/model/N_PCFG.py:759: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
      "  stat_rw = rw.std(-1).mean(-1) #if not isinstance(rw, int) else 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 100/? [00:24<00:00,  4.10it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "assert not args.debug, \"debug mode is not allowed in this version\"\n",
    "trainer = L.Trainer(\n",
    "    # limit_train_batches=100,\n",
    "    # # max_epochs=20,\n",
    "    # min_epochs = 5,\n",
    "    max_steps=max_steps,\n",
    "    min_steps=min_steps,\n",
    "    min_epochs=0,\n",
    "    val_check_interval=val_check_interval,\n",
    "    check_val_every_n_epoch=None,\n",
    "    gradient_clip_val=args.train.clip,\n",
    "    gradient_clip_algorithm=\"norm\",\n",
    "    callbacks=[\n",
    "        early_stop_callback,\n",
    "        TQDMProgressBar(refresh_rate=10),\n",
    "        best_sf1_checkpoint_callback if not args.analysis_mode and not args.corr_mode else saveall_checkpoint_callback,\n",
    "        rl_coeff_scheduler,\n",
    "        maxent_scheduler,\n",
    "    ],\n",
    "    logger=[],  # if not args.debug else None,\n",
    "    # devices=[args.rank],\n",
    "    inference_mode=False,\n",
    "    log_every_n_steps=10,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,          # Number of GPUs to use\n",
    "    # strategy=\"ddp\"      # Use Distributed Data Parallel\n",
    "    \n",
    ")\n",
    "# wandb_logger.watch(model, log_graph=False, log_freq=100)\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dl,\n",
    "    val_dataloaders=val_dl,\n",
    "    ckpt_path=args.continue_from,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1223/1223 [01:31<00:00, 13.41it/s]total samples 2412.0\n",
      "Testing DataLoader 0: 100%|██████████| 1223/1223 [01:31<00:00, 13.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test/avg_ll        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    -144.63084411621094    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/avg_ppl        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     843.4830322265625     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test/corpus_f1       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.22327956557273865    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">     test/sentence_f1      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2539501190185547     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test/avg_ll       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   -144.63084411621094   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/avg_ppl       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    843.4830322265625    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test/corpus_f1      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.22327956557273865   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m    test/sentence_f1     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2539501190185547    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ends. The best model on Spacy mode: \n",
      " [{'test/corpus_f1': 0.22327956557273865, 'test/sentence_f1': 0.2539501190185547, 'test/avg_ll': -144.63084411621094, 'test/avg_ppl': 843.4830322265625}]\n"
     ]
    }
   ],
   "source": [
    "if args.allow_ptb_eval:\n",
    "    model.set_test_mode(\"ptb\")\n",
    "    print(\n",
    "        \"Training ends. The best model on PTB mode: \\n\",\n",
    "        trainer.test(model, dataloaders=test_dl),\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "model.set_test_mode(mode=\"spacy\")\n",
    "print(\n",
    "    \"Training ends. The best model on Spacy mode: \\n\",\n",
    "    trainer.test(model, dataloaders=test_dl),\n",
    "    file=sys.stderr,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
